package consensus

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"io"
	"io/ioutil"
	"os"
	"path"
	"runtime"
	"testing"
	"time"

	"github.com/stretchr/testify/require"

	"github.com/tendermint/abci/example/dummy"
	crypto "github.com/tendermint/go-crypto"
	wire "github.com/tendermint/go-wire"
	auto "github.com/tendermint/tmlibs/autofile"
	cmn "github.com/tendermint/tmlibs/common"
	dbm "github.com/tendermint/tmlibs/db"

	cfg "github.com/tendermint/tendermint/config"
	"github.com/tendermint/tendermint/proxy"
	sm "github.com/tendermint/tendermint/state"
	"github.com/tendermint/tendermint/types"
	"github.com/tendermint/tmlibs/log"
)

var consensusReplayConfig *cfg.Config

func init() {
	consensusReplayConfig = ResetConfig("consensus_replay_test")
}

// These tests ensure we can always recover from failure at any part of the consensus process.
// There are two general failure scenarios: failure during consensus, and failure while applying the block.
// Only the latter interacts with the app and store,
// but the former has to deal with restrictions on re-use of priv_validator keys.
// The `WAL Tests` are for failures during the consensus;
// the `Handshake Tests` are for failures in applying the block.
// With the help of the WAL, we can recover from it all!

// NOTE: Files in this dir are generated by running the `build.sh` therein.
// It's a simple way to generate wals for a single block, or multiple blocks, with random transactions,
// and different part sizes. The output is not deterministic, and the stepChanges may need to be adjusted
// after running it (eg. sometimes small_block2 will have 5 block parts, sometimes 6).
// It should only have to be re-run if there is some breaking change to the consensus data structures (eg. blocks, votes)
// or to the behaviour of the app (eg. computes app hash differently)
var data_dir = path.Join(cmn.GoPath, "src/github.com/tendermint/tendermint/consensus", "test_data")

//------------------------------------------------------------------------------------------
// WAL Tests

// TODO: It would be better to verify explicitly which states we can recover from without the wal
// and which ones we need the wal for - then we'd also be able to only flush the
// wal writer when we need to, instead of with every message.

func startNewConsensusStateAndWaitForBlock(t *testing.T) {
	cs := fixedConsensusStateDummy(consensusReplayConfig, log.TestingLogger())

	bytes, _ := ioutil.ReadFile(cs.config.WalFile())
	t.Logf("====== WAL: \n\r%s\n", bytes)

	_, err := cs.Start()
	require.NoError(t, err)
	defer func() {
		cs.Stop()
		cs.Wait()
		cs = nil
	}()

	// This is just a signal that we haven't halted; its not something contained
	// in the WAL itself. Assuming the consensus state is running, replay of any
	// WAL, including the empty one, should eventually be followed by a new
	// block, or else something is wrong.
	newBlockCh := make(chan interface{}, 1)
	err = cs.eventBus.Subscribe(context.Background(), testSubscriber, types.EventQueryNewBlock, newBlockCh)
	require.NoError(t, err)
	select {
	case <-newBlockCh:
	case <-time.After(10 * time.Second):
		t.Fatalf("Timed out waiting for new block (see trace above)")
	}
}

// TestWALCrash uses ramdomly crashing WAL to test we can recover from any WAL
// failure.
func TestWALCrash(t *testing.T) {
	testCases := []struct {
		name   string
		initFn func(*ConsensusState, context.Context)
		doneFn func(*ConsensusState) bool
	}{
		{"empty block",
			func(cs *ConsensusState, ctx context.Context) {},
			func(cs *ConsensusState) bool { cs.mtx.Lock(); defer cs.mtx.Unlock(); return cs.Height > 1 }},
		{"non-empty block",
			func(cs *ConsensusState, ctx context.Context) {
				i := 0
				for {
					select {
					case <-ctx.Done():
						return
					default:
						cs.mtx.Lock()
						cs.mempool.CheckTx([]byte{byte(cs.Height), byte(i)}, nil)
						cs.mtx.Unlock()
						i++
					}
				}
			},
			func(cs *ConsensusState) bool { cs.mtx.Lock(); defer cs.mtx.Unlock(); return cs.Height > 1 }},
		{"non-empty block with smaller part size",
			func(cs *ConsensusState, ctx context.Context) {
				// cs.mtx.Lock()
				// cs.config.BlockPartSize = 512
				// cs.mtx.Unlock()
				i := 0
				for {
					select {
					case <-ctx.Done():
						return
					default:
						cs.mtx.Lock()
						cs.mempool.CheckTx([]byte{byte(cs.Height), byte(i)}, nil)
						cs.mtx.Unlock()
						i++
					}
				}
			},
			func(cs *ConsensusState) bool { cs.mtx.Lock(); defer cs.mtx.Unlock(); return cs.Height > 1 }},
		{"many blocks",
			func(cs *ConsensusState, ctx context.Context) {
				i := 0
				for {
					select {
					case <-ctx.Done():
						return
					default:
						cs.mtx.Lock()
						cs.mempool.CheckTx([]byte{byte(cs.Height), byte(i)}, nil)
						cs.mtx.Unlock()
						i++
					}
				}
			},
			func(cs *ConsensusState) bool { cs.mtx.Lock(); defer cs.mtx.Unlock(); return cs.Height > 3 }},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			crashWALandCheckLiveness(t, tc.initFn, tc.doneFn)
		})
	}
}

func crashWALandCheckLiveness(t *testing.T, initFn func(*ConsensusState, context.Context), doneFn func(*ConsensusState) bool) {
	walPaniced := make(chan error, 10)
	crashingWal := &randomlyCrashingWAL{panicCh: walPaniced, panicedFor: make(map[string]bool)}

	i := 1
LOOP:
	for {
		t.Logf("====== LOOP %d\n", i)
		cs := fixedConsensusStateDummy(consensusReplayConfig, log.NewNopLogger())

		ctx, cancel := context.WithCancel(context.Background())
		go initFn(cs, ctx)

		done := make(chan struct{})
		go func() {
			for {
				select {
				case <-ctx.Done():
					return
				case <-time.After(100 * time.Millisecond):
					if doneFn(cs) {
						close(done)
						return
					}
				}
			}
		}()

		// clean WAL file
		walFile := cs.config.WalFile()
		os.Remove(walFile)

		csWal, err := cs.OpenWAL(walFile)
		require.NoError(t, err)

		// set randomlyCrashingWAL
		crashingWal.cs = cs
		crashingWal.next = csWal
		cs.wal = crashingWal

		_, err = cs.Start()
		require.NoError(t, err)

		i++

		select {
		case err := <-walPaniced:
			t.Logf("WAL paniced: %v", err)

			// make sure we can make blocks after a crash
			startNewConsensusStateAndWaitForBlock(t)

			// unblock consensus state and stop it
			cs.Stop()

			cancel()
		case <-time.After(10 * time.Second):
			t.Fatal("doneFn never returned true")
		case <-done:
			break LOOP
		}
	}
}

// randomlyCrashingWAL is a WAL which crashes or rather simulate a crash during
// Save (before and after). It records a consensus step so it won't fail the
// second time for the same step.
type randomlyCrashingWAL struct {
	next       WAL
	cs         *ConsensusState
	panicCh    chan error
	panicedFor map[string]bool
}

// Save simulate WAL's crashing by sending an error to panicCh and then
// exiting the receiveRoutine.
func (w *randomlyCrashingWAL) Save(m WALMessage) {
	step := w.cs.StepString()
	// key is a combination of WALMessage type : msgInfo msg type : consensus step
	var key string
	switch m.(type) {
	case msgInfo:
		key = fmt.Sprintf("%T:%T:%v", m, m.(msgInfo).Msg, step)
	default:
		key = fmt.Sprintf("%T::%v", m, step)
	}

	if _, ok := w.panicedFor[key]; !ok { // panic during writing to WAL
		w.panicedFor[key] = true
		_, file, line, _ := runtime.Caller(1)
		w.panicCh <- fmt.Errorf("failed to write to WAL (msg type: %T, step: %v, fileline: %s:%d)", m, step, file, line)
		runtime.Goexit()
	} else {
		w.next.Save(m)
	}
}

func (w *randomlyCrashingWAL) Group() *auto.Group        { return w.next.Group() }
func (w *randomlyCrashingWAL) WriteEndHeight(height int) { w.next.WriteEndHeight(height) }
func (w *randomlyCrashingWAL) Start() (bool, error)      { return w.next.Start() }
func (w *randomlyCrashingWAL) Stop() bool                { return w.next.Stop() }
func (w *randomlyCrashingWAL) Wait()                     { w.next.Wait() }

//------------------------------------------------------------------------------------------
// Handshake Tests

var (
	NUM_BLOCKS = 6 // number of blocks in the test_data/many_blocks.cswal
	mempool    = types.MockMempool{}

	testPartSize int
)

//---------------------------------------
// Test handshake/replay

// 0 - all synced up
// 1 - saved block but app and state are behind
// 2 - save block and committed but state is behind
var modes = []uint{0, 1, 2}

// Sync from scratch
func TestHandshakeReplayAll(t *testing.T) {
	for _, m := range modes {
		testHandshakeReplay(t, 0, m)
	}
}

// Sync many, not from scratch
func TestHandshakeReplaySome(t *testing.T) {
	for _, m := range modes {
		testHandshakeReplay(t, 1, m)
	}
}

// Sync from lagging by one
func TestHandshakeReplayOne(t *testing.T) {
	for _, m := range modes {
		testHandshakeReplay(t, NUM_BLOCKS-1, m)
	}
}

// Sync from caught up
func TestHandshakeReplayNone(t *testing.T) {
	for _, m := range modes {
		testHandshakeReplay(t, NUM_BLOCKS, m)
	}
}

func writeWAL(walMsgs string) string {
	tempDir := os.TempDir()
	walDir := path.Join(tempDir, "/wal"+cmn.RandStr(12))
	walFile := path.Join(walDir, "wal")
	// Create WAL directory
	err := cmn.EnsureDir(walDir, 0700)
	if err != nil {
		panic(err)
	}
	// Write the needed WAL to file
	err = cmn.WriteFile(walFile, []byte(walMsgs), 0600)
	if err != nil {
		panic(err)
	}
	return walFile
}

// Make some blocks. Start a fresh app and apply nBlocks blocks. Then restart the app and sync it up with the remaining blocks
func testHandshakeReplay(t *testing.T, nBlocks int, mode uint) {
	config := ResetConfig("proxy_test_")

	// copy the many_blocks file
	walBody, err := cmn.ReadFile(path.Join(data_dir, "many_blocks.cswal"))
	if err != nil {
		t.Fatal(err)
	}
	walFile := writeWAL(string(walBody))
	config.Consensus.SetWalFile(walFile)

	privVal := types.LoadPrivValidator(config.PrivValidatorFile())
	testPartSize = config.Consensus.BlockPartSize

	wal, err := NewWAL(walFile, false)
	if err != nil {
		t.Fatal(err)
	}
	wal.SetLogger(log.TestingLogger())
	if _, err := wal.Start(); err != nil {
		t.Fatal(err)
	}
	chain, commits, err := makeBlockchainFromWAL(wal)
	if err != nil {
		t.Fatalf(err.Error())
	}

	state, store := stateAndStore(config, privVal.PubKey)
	store.chain = chain
	store.commits = commits

	// run the chain through state.ApplyBlock to build up the tendermint state
	latestAppHash := buildTMStateFromChain(config, state, chain, mode)

	// make a new client creator
	dummyApp := dummy.NewPersistentDummyApplication(path.Join(config.DBDir(), "2"))
	clientCreator2 := proxy.NewLocalClientCreator(dummyApp)
	if nBlocks > 0 {
		// run nBlocks against a new client to build up the app state.
		// use a throwaway tendermint state
		proxyApp := proxy.NewAppConns(clientCreator2, nil)
		state, _ := stateAndStore(config, privVal.PubKey)
		buildAppStateFromChain(proxyApp, state, chain, nBlocks, mode)
	}

	// now start the app using the handshake - it should sync
	handshaker := NewHandshaker(state, store)
	proxyApp := proxy.NewAppConns(clientCreator2, handshaker)
	if _, err := proxyApp.Start(); err != nil {
		t.Fatalf("Error starting proxy app connections: %v", err)
	}

	// get the latest app hash from the app
	res, err := proxyApp.Query().InfoSync()
	if err != nil {
		t.Fatal(err)
	}

	// the app hash should be synced up
	if !bytes.Equal(latestAppHash, res.LastBlockAppHash) {
		t.Fatalf("Expected app hashes to match after handshake/replay. got %X, expected %X", res.LastBlockAppHash, latestAppHash)
	}

	expectedBlocksToSync := NUM_BLOCKS - nBlocks
	if nBlocks == NUM_BLOCKS && mode > 0 {
		expectedBlocksToSync += 1
	} else if nBlocks > 0 && mode == 1 {
		expectedBlocksToSync += 1
	}

	if handshaker.NBlocks() != expectedBlocksToSync {
		t.Fatalf("Expected handshake to sync %d blocks, got %d", expectedBlocksToSync, handshaker.NBlocks())
	}
}

func applyBlock(st *sm.State, blk *types.Block, proxyApp proxy.AppConns) {
	err := st.ApplyBlock(types.NopEventBus{}, proxyApp.Consensus(), blk, blk.MakePartSet(testPartSize).Header(), mempool)
	if err != nil {
		panic(err)
	}
}

func buildAppStateFromChain(proxyApp proxy.AppConns,
	state *sm.State, chain []*types.Block, nBlocks int, mode uint) {
	// start a new app without handshake, play nBlocks blocks
	if _, err := proxyApp.Start(); err != nil {
		panic(err)
	}

	validators := types.TM2PB.Validators(state.Validators)
	proxyApp.Consensus().InitChainSync(validators)

	defer proxyApp.Stop()
	switch mode {
	case 0:
		for i := 0; i < nBlocks; i++ {
			block := chain[i]
			applyBlock(state, block, proxyApp)
		}
	case 1, 2:
		for i := 0; i < nBlocks-1; i++ {
			block := chain[i]
			applyBlock(state, block, proxyApp)
		}

		if mode == 2 {
			// update the dummy height and apphash
			// as if we ran commit but not
			applyBlock(state, chain[nBlocks-1], proxyApp)
		}
	}

}

func buildTMStateFromChain(config *cfg.Config, state *sm.State, chain []*types.Block, mode uint) []byte {
	// run the whole chain against this client to build up the tendermint state
	clientCreator := proxy.NewLocalClientCreator(dummy.NewPersistentDummyApplication(path.Join(config.DBDir(), "1")))
	proxyApp := proxy.NewAppConns(clientCreator, nil) // sm.NewHandshaker(config, state, store, ReplayLastBlock))
	if _, err := proxyApp.Start(); err != nil {
		panic(err)
	}
	defer proxyApp.Stop()

	validators := types.TM2PB.Validators(state.Validators)
	proxyApp.Consensus().InitChainSync(validators)

	var latestAppHash []byte

	switch mode {
	case 0:
		// sync right up
		for _, block := range chain {
			applyBlock(state, block, proxyApp)
		}

		latestAppHash = state.AppHash
	case 1, 2:
		// sync up to the penultimate as if we stored the block.
		// whether we commit or not depends on the appHash
		for _, block := range chain[:len(chain)-1] {
			applyBlock(state, block, proxyApp)
		}

		// apply the final block to a state copy so we can
		// get the right next appHash but keep the state back
		stateCopy := state.Copy()
		applyBlock(stateCopy, chain[len(chain)-1], proxyApp)
		latestAppHash = stateCopy.AppHash
	}

	return latestAppHash
}

//--------------------------
// utils for making blocks

func makeBlockchainFromWAL(wal WAL) ([]*types.Block, []*types.Commit, error) {
	// Search for height marker
	gr, found, err := wal.Group().Search("#ENDHEIGHT: ", makeHeightSearchFunc(0))
	if err != nil {
		return nil, nil, err
	}
	if !found {
		return nil, nil, errors.New(cmn.Fmt("WAL does not contain height %d.", 1))
	}
	defer gr.Close()

	// log.Notice("Build a blockchain by reading from the WAL")

	var blockParts *types.PartSet
	var blocks []*types.Block
	var commits []*types.Commit
	for {
		line, err := gr.ReadLine()
		if err != nil {
			if err == io.EOF {
				break
			} else {
				return nil, nil, err
			}
		}

		piece, err := readPieceFromWAL([]byte(line))
		if err != nil {
			return nil, nil, err
		}
		if piece == nil {
			continue
		}

		switch p := piece.(type) {
		case *types.PartSetHeader:
			// if its not the first one, we have a full block
			if blockParts != nil {
				var n int
				block := wire.ReadBinary(&types.Block{}, blockParts.GetReader(), types.MaxBlockSize, &n, &err).(*types.Block)
				blocks = append(blocks, block)
			}
			blockParts = types.NewPartSetFromHeader(*p)
		case *types.Part:
			_, err := blockParts.AddPart(p, false)
			if err != nil {
				return nil, nil, err
			}
		case *types.Vote:
			if p.Type == types.VoteTypePrecommit {
				commit := &types.Commit{
					BlockID:    p.BlockID,
					Precommits: []*types.Vote{p},
				}
				commits = append(commits, commit)
			}
		}
	}
	// grab the last block too
	var n int
	block := wire.ReadBinary(&types.Block{}, blockParts.GetReader(), types.MaxBlockSize, &n, &err).(*types.Block)
	blocks = append(blocks, block)
	return blocks, commits, nil
}

func readPieceFromWAL(msgBytes []byte) (interface{}, error) {
	// Skip over empty and meta lines
	if len(msgBytes) == 0 || msgBytes[0] == '#' {
		return nil, nil
	}
	var err error
	var msg TimedWALMessage
	wire.ReadJSON(&msg, msgBytes, &err)
	if err != nil {
		fmt.Println("MsgBytes:", msgBytes, string(msgBytes))
		return nil, fmt.Errorf("Error reading json data: %v", err)
	}

	// for logging
	switch m := msg.Msg.(type) {
	case msgInfo:
		switch msg := m.Msg.(type) {
		case *ProposalMessage:
			return &msg.Proposal.BlockPartsHeader, nil
		case *BlockPartMessage:
			return msg.Part, nil
		case *VoteMessage:
			return msg.Vote, nil
		}
	}
	return nil, nil
}

// fresh state and mock store
func stateAndStore(config *cfg.Config, pubKey crypto.PubKey) (*sm.State, *mockBlockStore) {
	stateDB := dbm.NewMemDB()
	state := sm.MakeGenesisStateFromFile(stateDB, config.GenesisFile())
	state.SetLogger(log.TestingLogger().With("module", "state"))

	store := NewMockBlockStore(config)
	return state, store
}

//----------------------------------
// mock block store

type mockBlockStore struct {
	config  *cfg.Config
	chain   []*types.Block
	commits []*types.Commit
}

// TODO: NewBlockStore(db.NewMemDB) ...
func NewMockBlockStore(config *cfg.Config) *mockBlockStore {
	return &mockBlockStore{config, nil, nil}
}

func (bs *mockBlockStore) Height() int                       { return len(bs.chain) }
func (bs *mockBlockStore) LoadBlock(height int) *types.Block { return bs.chain[height-1] }
func (bs *mockBlockStore) LoadBlockMeta(height int) *types.BlockMeta {
	block := bs.chain[height-1]
	return &types.BlockMeta{
		BlockID: types.BlockID{block.Hash(), block.MakePartSet(bs.config.Consensus.BlockPartSize).Header()},
		Header:  block.Header,
	}
}
func (bs *mockBlockStore) LoadBlockPart(height int, index int) *types.Part { return nil }
func (bs *mockBlockStore) SaveBlock(block *types.Block, blockParts *types.PartSet, seenCommit *types.Commit) {
}
func (bs *mockBlockStore) LoadBlockCommit(height int) *types.Commit {
	return bs.commits[height-1]
}
func (bs *mockBlockStore) LoadSeenCommit(height int) *types.Commit {
	return bs.commits[height-1]
}
